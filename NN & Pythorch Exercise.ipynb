{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PyTorch, for building and training the model \n",
    "\n",
    "Loads a medical databset about breast cancer \n",
    "\n",
    "The train_test_split, a tool to split our data into training and test sets.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing an example of how Pytorch handle the gradient calculation, assuming the model and loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([6.0], requires_grad=True)\n",
    "\n",
    "# Define a function y = x^2\n",
    "y = x ** 2 + 1\n",
    "\n",
    "# Compute the gradient\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example creates a tensor x with the value of 6. \n",
    "\n",
    "The function tells Pytorch to track gradient or slope for this variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing further an example with loss function. Assuming we want to learn y = 2x + 1. Tnerefore, 2 and 1 is our target learnable parameter. Assuming y = wx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, w is tensor([1.], requires_grad=True)\n",
      "Before optimization, b is tensor([0.], requires_grad=True)\n",
      "Gradient of w: tensor([-13.3333])\n",
      "Gradient of b: tensor([-6.])\n",
      "After optimization, w is tensor([1.1000], requires_grad=True)\n",
      "After optimization, b is tensor([0.1000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\n",
    "y = torch.tensor([3.0, 5.0, 7.0], requires_grad=False)  # We give three observations. i.e., when x = 1, y = 3; x =2, y = 5\n",
    "\n",
    "# Initialize weights with requires_grad=True so we can compute gradients\n",
    "w = torch.tensor([1.0], requires_grad=True)  # initial guess for weight\n",
    "b = torch.tensor([0.0], requires_grad=True)  # initial guess for bias\n",
    "print(f'Before optimization, w is {w}')\n",
    "print(f'Before optimization, b is {b}')\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam([w,b], lr = 0.1)\n",
    "y_pred = w*x+b\n",
    "optimizer.zero_grad() # we reset the optimizer here\n",
    "loss = ((y_pred - y) ** 2).mean() # calculate the loss value \n",
    "loss.backward() # as shown before, this step calculates the gradient of both w and b\n",
    "optimizer.step()   # it performs the change of w and b\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradient of w:\", w.grad)  # ∂loss/∂w\n",
    "print(\"Gradient of b:\", b.grad)  # ∂loss/∂b\n",
    "\n",
    "print(f'After optimization, w is {w}')\n",
    "print(f'After optimization, b is {b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model starts with with w=1, b=0 \n",
    "\n",
    "It calculated how to adjust w and b to reduce error. \n",
    "\n",
    "It update w to 1.1000 and b to 0.1000. \n",
    "\n",
    "Optimizing the model will eventually find the best w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### this is the 0 times try\n",
      "current loss is 7.829999923706055\n",
      "After optimization, w is tensor([1.2000], requires_grad=True)\n",
      "After optimization, b is tensor([0.2000], requires_grad=True)\n",
      "##### this is the 1 times try\n",
      "current loss is 6.186666011810303\n",
      "After optimization, w is tensor([1.3000], requires_grad=True)\n",
      "After optimization, b is tensor([0.3000], requires_grad=True)\n",
      "##### this is the 2 times try\n",
      "current loss is 4.736665725708008\n",
      "After optimization, w is tensor([1.4000], requires_grad=True)\n",
      "After optimization, b is tensor([0.4000], requires_grad=True)\n",
      "##### this is the 3 times try\n",
      "current loss is 3.479998826980591\n",
      "After optimization, w is tensor([1.5000], requires_grad=True)\n",
      "After optimization, b is tensor([0.5000], requires_grad=True)\n",
      "##### this is the 4 times try\n",
      "current loss is 2.41666579246521\n",
      "After optimization, w is tensor([1.6000], requires_grad=True)\n",
      "After optimization, b is tensor([0.6000], requires_grad=True)\n",
      "##### this is the 5 times try\n",
      "current loss is 1.5466665029525757\n",
      "After optimization, w is tensor([1.7000], requires_grad=True)\n",
      "After optimization, b is tensor([0.7000], requires_grad=True)\n",
      "##### this is the 6 times try\n",
      "current loss is 0.8699995875358582\n",
      "After optimization, w is tensor([1.8000], requires_grad=True)\n",
      "After optimization, b is tensor([0.8000], requires_grad=True)\n",
      "##### this is the 7 times try\n",
      "current loss is 0.3866659700870514\n",
      "After optimization, w is tensor([1.9000], requires_grad=True)\n",
      "After optimization, b is tensor([0.9000], requires_grad=True)\n",
      "##### this is the 8 times try\n",
      "current loss is 0.0966663584113121\n",
      "After optimization, w is tensor([2.0000], requires_grad=True)\n",
      "After optimization, b is tensor([1.0000], requires_grad=True)\n",
      "##### this is the 9 times try\n",
      "current loss is 3.78956116703702e-13\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n",
      "##### this is the 10 times try\n",
      "current loss is 0.09552498906850815\n",
      "After optimization, w is tensor([2.0004], requires_grad=True)\n",
      "After optimization, b is tensor([1.0010], requires_grad=True)\n",
      "##### this is the 11 times try\n",
      "current loss is 3.420349457883276e-06\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n",
      "##### this is the 12 times try\n",
      "current loss is 0.09552469849586487\n",
      "After optimization, w is tensor([2.0004], requires_grad=True)\n",
      "After optimization, b is tensor([1.0010], requires_grad=True)\n",
      "##### this is the 13 times try\n",
      "current loss is 3.4205768315587193e-06\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n",
      "##### this is the 14 times try\n",
      "current loss is 0.09552445262670517\n",
      "After optimization, w is tensor([2.0004], requires_grad=True)\n",
      "After optimization, b is tensor([1.0010], requires_grad=True)\n",
      "##### this is the 15 times try\n",
      "current loss is 3.4220865927636623e-06\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n",
      "##### this is the 16 times try\n",
      "current loss is 0.09552440792322159\n",
      "After optimization, w is tensor([2.0004], requires_grad=True)\n",
      "After optimization, b is tensor([1.0010], requires_grad=True)\n",
      "##### this is the 17 times try\n",
      "current loss is 3.423597263463307e-06\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n",
      "##### this is the 18 times try\n",
      "current loss is 0.09552416950464249\n",
      "After optimization, w is tensor([2.0004], requires_grad=True)\n",
      "After optimization, b is tensor([1.0010], requires_grad=True)\n",
      "##### this is the 19 times try\n",
      "current loss is 3.4238246371387504e-06\n",
      "After optimization, w is tensor([1.9004], requires_grad=True)\n",
      "After optimization, b is tensor([0.9010], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    optimizer = torch.optim.Adam([w,b], lr = 0.1)\n",
    "    y_pred = w*x+b\n",
    "    optimizer.zero_grad() # we reset the optimizer here\n",
    "    loss = ((y_pred - y) ** 2).mean() # calculate the loss value \n",
    "    print(f\"##### this is the {i} times try\")\n",
    "    print(f\"current loss is {loss}\")\n",
    "    loss.backward() # as shown before, this step calculates the gradient of both w and b\n",
    "    optimizer.step()   # it performs the change of w and b\n",
    "    print(f'After optimization, w is {w}')\n",
    "    print(f'After optimization, b is {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.799e+01 1.038e+01 1.228e+02 1.001e+03 1.184e-01 2.776e-01 3.001e-01\n",
      "  1.471e-01 2.419e-01 7.871e-02 1.095e+00 9.053e-01 8.589e+00 1.534e+02\n",
      "  6.399e-03 4.904e-02 5.373e-02 1.587e-02 3.003e-02 6.193e-03 2.538e+01\n",
      "  1.733e+01 1.846e+02 2.019e+03 1.622e-01 6.656e-01 7.119e-01 2.654e-01\n",
      "  4.601e-01 1.189e-01]\n",
      " [2.057e+01 1.777e+01 1.329e+02 1.326e+03 8.474e-02 7.864e-02 8.690e-02\n",
      "  7.017e-02 1.812e-01 5.667e-02 5.435e-01 7.339e-01 3.398e+00 7.408e+01\n",
      "  5.225e-03 1.308e-02 1.860e-02 1.340e-02 1.389e-02 3.532e-03 2.499e+01\n",
      "  2.341e+01 1.588e+02 1.956e+03 1.238e-01 1.866e-01 2.416e-01 1.860e-01\n",
      "  2.750e-01 8.902e-02]\n",
      " [1.969e+01 2.125e+01 1.300e+02 1.203e+03 1.096e-01 1.599e-01 1.974e-01\n",
      "  1.279e-01 2.069e-01 5.999e-02 7.456e-01 7.869e-01 4.585e+00 9.403e+01\n",
      "  6.150e-03 4.006e-02 3.832e-02 2.058e-02 2.250e-02 4.571e-03 2.357e+01\n",
      "  2.553e+01 1.525e+02 1.709e+03 1.444e-01 4.245e-01 4.504e-01 2.430e-01\n",
      "  3.613e-01 8.758e-02]\n",
      " [1.142e+01 2.038e+01 7.758e+01 3.861e+02 1.425e-01 2.839e-01 2.414e-01\n",
      "  1.052e-01 2.597e-01 9.744e-02 4.956e-01 1.156e+00 3.445e+00 2.723e+01\n",
      "  9.110e-03 7.458e-02 5.661e-02 1.867e-02 5.963e-02 9.208e-03 1.491e+01\n",
      "  2.650e+01 9.887e+01 5.677e+02 2.098e-01 8.663e-01 6.869e-01 2.575e-01\n",
      "  6.638e-01 1.730e-01]\n",
      " [2.029e+01 1.434e+01 1.351e+02 1.297e+03 1.003e-01 1.328e-01 1.980e-01\n",
      "  1.043e-01 1.809e-01 5.883e-02 7.572e-01 7.813e-01 5.438e+00 9.444e+01\n",
      "  1.149e-02 2.461e-02 5.688e-02 1.885e-02 1.756e-02 5.115e-03 2.254e+01\n",
      "  1.667e+01 1.522e+02 1.575e+03 1.374e-01 2.050e-01 4.000e-01 1.625e-01\n",
      "  2.364e-01 7.678e-02]\n",
      " [1.245e+01 1.570e+01 8.257e+01 4.771e+02 1.278e-01 1.700e-01 1.578e-01\n",
      "  8.089e-02 2.087e-01 7.613e-02 3.345e-01 8.902e-01 2.217e+00 2.719e+01\n",
      "  7.510e-03 3.345e-02 3.672e-02 1.137e-02 2.165e-02 5.082e-03 1.547e+01\n",
      "  2.375e+01 1.034e+02 7.416e+02 1.791e-01 5.249e-01 5.355e-01 1.741e-01\n",
      "  3.985e-01 1.244e-01]\n",
      " [1.825e+01 1.998e+01 1.196e+02 1.040e+03 9.463e-02 1.090e-01 1.127e-01\n",
      "  7.400e-02 1.794e-01 5.742e-02 4.467e-01 7.732e-01 3.180e+00 5.391e+01\n",
      "  4.314e-03 1.382e-02 2.254e-02 1.039e-02 1.369e-02 2.179e-03 2.288e+01\n",
      "  2.766e+01 1.532e+02 1.606e+03 1.442e-01 2.576e-01 3.784e-01 1.932e-01\n",
      "  3.063e-01 8.368e-02]\n",
      " [1.371e+01 2.083e+01 9.020e+01 5.779e+02 1.189e-01 1.645e-01 9.366e-02\n",
      "  5.985e-02 2.196e-01 7.451e-02 5.835e-01 1.377e+00 3.856e+00 5.096e+01\n",
      "  8.805e-03 3.029e-02 2.488e-02 1.448e-02 1.486e-02 5.412e-03 1.706e+01\n",
      "  2.814e+01 1.106e+02 8.970e+02 1.654e-01 3.682e-01 2.678e-01 1.556e-01\n",
      "  3.196e-01 1.151e-01]\n",
      " [1.300e+01 2.182e+01 8.750e+01 5.198e+02 1.273e-01 1.932e-01 1.859e-01\n",
      "  9.353e-02 2.350e-01 7.389e-02 3.063e-01 1.002e+00 2.406e+00 2.432e+01\n",
      "  5.731e-03 3.502e-02 3.553e-02 1.226e-02 2.143e-02 3.749e-03 1.549e+01\n",
      "  3.073e+01 1.062e+02 7.393e+02 1.703e-01 5.401e-01 5.390e-01 2.060e-01\n",
      "  4.378e-01 1.072e-01]\n",
      " [1.246e+01 2.404e+01 8.397e+01 4.759e+02 1.186e-01 2.396e-01 2.273e-01\n",
      "  8.543e-02 2.030e-01 8.243e-02 2.976e-01 1.599e+00 2.039e+00 2.394e+01\n",
      "  7.149e-03 7.217e-02 7.743e-02 1.432e-02 1.789e-02 1.008e-02 1.509e+01\n",
      "  4.068e+01 9.765e+01 7.114e+02 1.853e-01 1.058e+00 1.105e+00 2.210e-01\n",
      "  4.366e-01 2.075e-01]]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Load the breast cancer dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "print(X[:10, :])\n",
    "print(y[:10])\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is a common process in machine learning, which normalize all the input data into [0-1] or [-1, 1].\n",
    "\n",
    "It has several main benefits:\n",
    "\n",
    "1. Faster Convergence during Training\n",
    "\n",
    "2. Enhanced Stability of the Optimization Process\n",
    "\n",
    "3. Improved Model Accuracy\n",
    "\n",
    "package sklearn provides a useful function for different types of normalization. https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a scaler object and the tool adjusts the features (columns of data) so that they have a mean of 0 and SD of 1. \n",
    "\n",
    "The data is split into training set (80%) and testing set (20%). \n",
    "\n",
    "THe random_state of 40 ensure the split is repeatable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the variables before are stored using Numpy package. As just discussed, they are not designed for Neural Network computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test different data slicing here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2719, -0.1214, -0.9706, -0.1072, -0.1611,  1.6367,  0.6625, -0.6241,\n",
      "        -0.0986, -0.3542, -0.3457, -0.8655,  1.4975, -0.6780, -0.9393, -0.0788,\n",
      "        -0.2634, -0.8712, -0.9592, -0.4735, -0.5332, -0.5076, -0.2974,  0.2109,\n",
      "         0.6086, -0.6752,  0.0803, -0.6581, -1.0813,  1.4379,  0.0604, -0.0305,\n",
      "        -0.2719, -0.3628, -0.8144,  1.7134, -0.3344,  0.9011, -0.8087, -0.7689,\n",
      "        -0.1100, -0.6922,  1.5089, -0.4281,  0.9494, -1.2517,  1.7503, -0.2605,\n",
      "        -1.0330, -0.6468, -0.4735, -0.7547, -0.8172, -1.2665,  1.1084,  0.2848,\n",
      "         3.1505, -0.6610, -1.0984,  0.3331, -0.4707,  0.3387, -0.6951, -0.5389,\n",
      "        -0.1668, -1.0274, -1.0898, -0.1810, -0.7149,  0.0207, -1.6846, -0.4821,\n",
      "        -0.8257,  1.8241,  0.0349, -0.9280, -0.5076, -0.7490,  0.5404,  0.3984,\n",
      "        -0.2605,  1.3328, -0.3514,  0.2763,  0.7534, -1.1211,  1.5401,  1.7191,\n",
      "        -0.8655, -0.3315, -0.7121,  0.6029, -0.9848,  1.0800,  1.2306,  0.0405,\n",
      "         1.3186, -1.1154, -1.6780, -0.4934,  0.6313,  1.1709, -0.4508, -0.3258,\n",
      "         1.5799, -0.9450,  0.2479,  1.7418,  0.9778,  1.6083, -1.2455,  0.1854,\n",
      "        -0.3116, -0.4707, -0.3486,  1.7049, -0.1924, -0.5928,  0.8386, -0.6042,\n",
      "         0.2081,  0.8699, -0.5275,  0.4893, -0.1469,  0.9153, -1.4909,  0.5802,\n",
      "        -1.2355,  1.7247, -0.7661,  0.5376,  0.4495, -0.2719, -1.0984,  1.9974,\n",
      "         0.4467,  0.4609, -0.3060, -0.4253, -0.3202, -0.3826, -0.9024, -1.5497,\n",
      "        -0.1356, -0.3599, -1.3801, -1.3724, -0.4054,  1.2732, -1.1012,  0.9863,\n",
      "         0.3018, -0.6354, -1.2418,  0.0377,  1.7645,  1.2788,  1.4350, -0.1469,\n",
      "         0.0831, -0.5190, -0.2066,  1.1624, -0.8768, -1.2074, -1.3131, -0.1270,\n",
      "        -0.6752, -0.5019,  0.1399, -0.7717, -0.6865, -0.6354, -0.4650, -0.8768,\n",
      "        -0.3883, -0.4451, -1.0018, -0.0759, -0.8712, -0.3514,  0.7278,  1.5515,\n",
      "        -0.4281, -0.1838,  1.0289,  1.8355,  0.4694, -0.0191,  2.5796, -0.8513,\n",
      "        -1.2486, -1.2841,  2.0570,  1.2391, -0.6212, -0.2350, -0.3684, -1.2455,\n",
      "        -1.2074,  1.5401, -0.6212, -0.1952,  1.0573,  1.0971, -1.5647, -0.1242,\n",
      "         2.1650, -0.8513,  1.5117, -0.3684,  3.7753,  0.1002, -0.4395,  1.6935,\n",
      "        -1.1268,  0.1456,  2.1536, -0.5133, -1.3412, -0.3542, -1.8172, -0.5445,\n",
      "        -0.2435, -0.7263, -0.2520,  0.5688, -1.0103, -0.5871, -0.7348, -0.0475,\n",
      "        -0.5190, -0.6865,  1.4606, -0.6723,  0.7420, -0.9876,  0.6739,  0.0263,\n",
      "         1.8014,  0.1598, -0.2889, -0.4764, -0.7831, -0.7178, -0.0788,  0.7165,\n",
      "        -1.8084, -0.5900, -0.4196, -0.8115,  1.3669,  2.8750, -1.0274,  1.3896,\n",
      "         0.1343, -1.4071,  1.3839,  1.1027, -0.3855,  0.1911,  1.1879, -0.0248,\n",
      "         0.0973,  0.2450, -0.2122, -0.5417,  1.4975,  1.8383, -0.2066,  2.5967,\n",
      "        -0.5332, -0.1895,  1.0090, -0.2236,  0.2621,  2.6648,  0.0888, -1.2327,\n",
      "         1.9803,  0.8329,  0.5518, -0.0646,  0.4353,  3.2953,  0.2819, -0.7433,\n",
      "        -0.5957, -0.4111, -1.2952,  0.1456, -0.5161, -0.6184,  1.1879,  1.4294,\n",
      "        -0.1526, -0.5275, -0.2804,  3.7185,  1.8298, -0.7916, -1.2622, -0.9138,\n",
      "        -0.0276, -0.1782, -0.7348,  0.6171,  1.2874,  0.2337, -0.1867,  0.2251,\n",
      "         0.2706,  0.8926, -0.8002, -1.1995, -0.3997,  0.2763, -0.7575, -0.2435,\n",
      "        -0.3173,  0.4211, -0.3770, -0.6326,  0.8557, -0.1327, -0.8030,  0.1059,\n",
      "         1.7929, -0.6411,  2.1252, -0.5502, -0.0532, -0.0418, -0.3599,  2.5455,\n",
      "         1.2277, -0.2889,  1.7560, -0.7462, -0.0731, -0.8058, -0.0702, -0.9223,\n",
      "        -1.4480, -0.9450,  1.8497,  0.4609, -0.7973, -0.6496, -0.0873, -0.9052,\n",
      "        -1.5707, -2.0296, -1.1637, -0.1327, -0.5019, -0.3741,  1.1624, -0.6865,\n",
      "         1.0743, -0.8484, -0.3514, -0.6070, -1.4704, -0.8456, -0.1015, -0.3372,\n",
      "        -1.5704, -0.5758,  0.7023,  1.2845,  0.2450, -0.1696,  0.1797, -0.2151,\n",
      "        -0.1980, -1.0330,  0.1371,  1.7191,  0.8301, -0.3883, -0.6723, -0.7945,\n",
      "         1.9292, -1.4562,  0.8216, -1.5781, -0.5843,  0.2649,  1.3300, -0.1185,\n",
      "        -0.7405,  0.6000, -0.6241,  1.0857,  1.9349,  0.9693, -0.6780,  0.3103,\n",
      "        -1.1580,  0.0178,  0.3274, -1.0359, -0.3855, -0.5076,  0.8727,  0.3899,\n",
      "         0.1286, -0.2406, -0.4764, -0.0731,  1.4123,  0.1655, -0.6496,  0.2109,\n",
      "        -0.0646,  0.0831, -1.5318, -0.7604,  2.3126, -0.1441, -1.0472, -1.2642,\n",
      "         1.5770,  0.2195,  1.5344, -1.1239,  0.1740,  1.5515, -1.3616,  1.8327,\n",
      "        -1.3435,  0.3785, -0.4849,  0.3785,  0.3444,  0.2592,  0.0462,  0.8187,\n",
      "        -0.3571, -0.3202,  0.6057, -0.0930,  0.3529, -1.0103, -0.7405, -0.6979,\n",
      "        -0.3571, -0.2634, -0.8967,  0.2393, -0.1185,  1.5344, -0.0049],\n",
      "       dtype=torch.float64)\n",
      "tensor([-0.2719, -0.1465, -0.2467, -0.3421,  1.3834,  0.3555,  0.4244,  0.6315,\n",
      "         1.1551,  0.7049, -0.4261, -0.5863, -0.4797, -0.3539, -0.1697, -0.1184,\n",
      "        -0.0943,  0.0574, -0.3768, -0.0575, -0.1241,  0.3701, -0.1329, -0.2130,\n",
      "         2.0266,  1.0327,  1.0958,  1.4343,  1.6166,  1.8816],\n",
      "       dtype=torch.float64)\n",
      "tensor(-0.2719, dtype=torch.float64)\n",
      "tensor([0, 1, 1, 1, 1, 0, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:, 0])\n",
    "print(X_train[0, :])\n",
    "print(X_train[0,0])\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(X_train[:, 0]) - All the rows in the first column\n",
    "\n",
    "(X_train[0, :]) - The first row and all column\n",
    "\n",
    "(X_train[0,0]) - The top left value in the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable can also be put into either GPU or CPU, by the following commands. Remember, a variable in CPU cannot compute with a variable in GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu') # if you want to put it to gpu, use 'cuda'\n",
    "x_GPU = X_train.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CancerNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(30, 10) # 30 input features (breast cancer).\n",
    "        self.fc2 = nn.Linear(10, 1)  # 1 output (0 or 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "model = CancerNet()\n",
    "mode = model.to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model looks at 30 medical measurements of a breast tumor and decide if \n",
    "\n",
    "0 = cancerous \n",
    "\n",
    "1= not cancerous \n",
    "\n",
    "Takes 30 numbers and passes them through 10 brain cells (neurons) to find the patterns.\n",
    "\n",
    "Sigmoid converts the output to a 0-1 probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() # Binary Cross-Entropy Loss. It measures how wrong the model's predictions are i.e, compares predicted probabilities and true labels .\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # we have seen this optimizer before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(torch.float32)\n",
    "y_train = y_train.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], Loss: 0.0131\n",
      "Epoch [20/500], Loss: 0.0121\n",
      "Epoch [30/500], Loss: 0.0112\n",
      "Epoch [40/500], Loss: 0.0104\n",
      "Epoch [50/500], Loss: 0.0097\n",
      "Epoch [60/500], Loss: 0.0090\n",
      "Epoch [70/500], Loss: 0.0084\n",
      "Epoch [80/500], Loss: 0.0078\n",
      "Epoch [90/500], Loss: 0.0073\n",
      "Epoch [100/500], Loss: 0.0068\n",
      "Epoch [110/500], Loss: 0.0064\n",
      "Epoch [120/500], Loss: 0.0060\n",
      "Epoch [130/500], Loss: 0.0056\n",
      "Epoch [140/500], Loss: 0.0053\n",
      "Epoch [150/500], Loss: 0.0049\n",
      "Epoch [160/500], Loss: 0.0047\n",
      "Epoch [170/500], Loss: 0.0044\n",
      "Epoch [180/500], Loss: 0.0041\n",
      "Epoch [190/500], Loss: 0.0039\n",
      "Epoch [200/500], Loss: 0.0037\n",
      "Epoch [210/500], Loss: 0.0035\n",
      "Epoch [220/500], Loss: 0.0033\n",
      "Epoch [230/500], Loss: 0.0031\n",
      "Epoch [240/500], Loss: 0.0030\n",
      "Epoch [250/500], Loss: 0.0028\n",
      "Epoch [260/500], Loss: 0.0027\n",
      "Epoch [270/500], Loss: 0.0026\n",
      "Epoch [280/500], Loss: 0.0024\n",
      "Epoch [290/500], Loss: 0.0023\n",
      "Epoch [300/500], Loss: 0.0022\n",
      "Epoch [310/500], Loss: 0.0021\n",
      "Epoch [320/500], Loss: 0.0020\n",
      "Epoch [330/500], Loss: 0.0019\n",
      "Epoch [340/500], Loss: 0.0019\n",
      "Epoch [350/500], Loss: 0.0018\n",
      "Epoch [360/500], Loss: 0.0017\n",
      "Epoch [370/500], Loss: 0.0016\n",
      "Epoch [380/500], Loss: 0.0016\n",
      "Epoch [390/500], Loss: 0.0015\n",
      "Epoch [400/500], Loss: 0.0015\n",
      "Epoch [410/500], Loss: 0.0014\n",
      "Epoch [420/500], Loss: 0.0014\n",
      "Epoch [430/500], Loss: 0.0013\n",
      "Epoch [440/500], Loss: 0.0013\n",
      "Epoch [450/500], Loss: 0.0012\n",
      "Epoch [460/500], Loss: 0.0012\n",
      "Epoch [470/500], Loss: 0.0011\n",
      "Epoch [480/500], Loss: 0.0011\n",
      "Epoch [490/500], Loss: 0.0011\n",
      "Epoch [500/500], Loss: 0.0010\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad() #remember this training loop, it is the most standard way to train the model, i.e. adjusting the paraemters until loss value is minimal. \n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs.ravel(), y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "# we actually can let the training process stop if the loss value is lower than a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.FloatTensor'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.type()\n",
    "outputs.type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model studies the entire dataset 100 times\n",
    "\n",
    "The loss keeps decreasing from 0.0601 to 0.0299. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.34\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test.to(torch.float32))\n",
    "    predicted = torch.argmax(test_outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No grad tells the model not to learn from the data, just what it knows. \n",
    "\n",
    "Predicted functions converts probabilities to clear decision. \n",
    "\n",
    "Accuracy function compares predictions to real real diagnosis (y_test). \n",
    "\n",
    "The test accuracy is very poor. The model correctly cancerous/non-cancerous tumor 34% of the time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Testenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
